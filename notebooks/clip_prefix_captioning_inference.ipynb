{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip_prefix_captioning_inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "pycharm-98db7c03",
      "language": "python",
      "display_name": "PyCharm (cvpr22)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1eb939f6352f4063808350cc4a97beae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_56c9a56fb276438ab3f6b120193deb42",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_873ac85e2658404ea6b284707824a07d",
              "IPY_MODEL_bcd00181175a4b739afc7a5eb30a235b",
              "IPY_MODEL_2094ef62bd9547fcb133f565818495fa"
            ]
          }
        },
        "56c9a56fb276438ab3f6b120193deb42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "873ac85e2658404ea6b284707824a07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a2eb4632f5d477182fcbf0ebcd7eb5b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93c09ca8d2ab40d381163b41c3bd345d"
          }
        },
        "bcd00181175a4b739afc7a5eb30a235b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0585589cf7b49098b0905ed10d96ecf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90bddc0c0d164e93996d0340f45bb9b5"
          }
        },
        "2094ef62bd9547fcb133f565818495fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5dc2c242f1594631a573bdbe4ace5a97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 523M/523M [00:20\u0026lt;00:00, 25.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ab34ffb29124fe9a2bf8b2aadf8c514"
          }
        },
        "5a2eb4632f5d477182fcbf0ebcd7eb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93c09ca8d2ab40d381163b41c3bd345d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0585589cf7b49098b0905ed10d96ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90bddc0c0d164e93996d0340f45bb9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5dc2c242f1594631a573bdbe4ace5a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ab34ffb29124fe9a2bf8b2aadf8c514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfpGaz27IWs",
        "outputId": "ebf43909-76e1-4c4a-e387-3501e9df9c4c",
        "pycharm": {}
      },
      "source": "#@title Install    \n!pip install transformers\n! pip install git+https://github.com/openai/CLIP.git\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml\u003e\u003d5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting huggingface-hub\u003e\u003d0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers\u003c0.11,\u003e\u003d0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm\u003e\u003d4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!\u003d2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy\u003e\u003d1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging\u003e\u003d20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e\u003d0.0.17-\u003etransformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing\u003e\u003d2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e\u003d20.0-\u003etransformers) (2.4.7)\n",
            "Requirement already satisfied: zipp\u003e\u003d0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!\u003d1.25.0,!\u003d1.25.1,\u003c1.26,\u003e\u003d1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n",
            "Requirement already satisfied: certifi\u003e\u003d2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet\u003c4,\u003e\u003d3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n",
            "Requirement already satisfied: idna\u003c3,\u003e\u003d2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-e9pft9oj\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-e9pft9oj\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip\u003d\u003d1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip\u003d\u003d1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip\u003d\u003d1.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip\u003d\u003d1.0) (0.10.0+cu111)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy-\u003eclip\u003d\u003d1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch-\u003eclip\u003d\u003d1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pillow\u003e\u003d5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003eclip\u003d\u003d1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003eclip\u003d\u003d1.0) (1.19.5)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename\u003dclip-1.0-py3-none-any.whl size\u003d1369090 sha256\u003d48b7ac167d3631347368e3ba02d8cf5c319ed9414532467ba8eb1e2ee5af3786\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_wqq4nr7/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename\u003dftfy-6.0.3-py3-none-any.whl size\u003d41933 sha256\u003d9ce1d67ee2d04149250e7246335dad1a3f1c8b9bd41d07182aa7f04a66c48abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built clip ftfy\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqE3Fj5-uYSR",
        "cellView": "form",
        "pycharm": {}
      },
      "source": [
        "#@title Drive Downloader\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive \u003d True #@param {type:\"boolean\"}  \n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive \u003d use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth \u003d GoogleAuth()\n",
        "        gauth.credentials \u003d GoogleCredentials.get_application_default()\n",
        "        self.drive \u003d GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_id, file_dst):\n",
        "        if self.use_pydrive:\n",
        "            downloaded \u003d self.drive.CreateFile({\u0027id\u0027:file_id})\n",
        "            downloaded.FetchMetadata(fetch_all\u003dTrue)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader \u003d Downloader(download_with_pydrive)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OArDkm_24w4L",
        "cellView": "form",
        "pycharm": {}
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "from IPython.display import Image \n",
        "\n",
        "\n",
        "N \u003d type(None)\n",
        "V \u003d np.array\n",
        "ARRAY \u003d np.ndarray\n",
        "ARRAYS \u003d Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS \u003d Union[Tuple[V, ...], List[V]]\n",
        "VN \u003d Union[V, N]\n",
        "VNS \u003d Union[VS, N]\n",
        "T \u003d torch.Tensor\n",
        "TS \u003d Union[Tuple[T, ...], List[T]]\n",
        "TN \u003d Optional[T]\n",
        "TNS \u003d Union[Tuple[TN, ...], List[TN]]\n",
        "TSN \u003d Optional[TS]\n",
        "TA \u003d Union[T, ARRAY]\n",
        "\n",
        "\n",
        "D \u003d torch.device\n",
        "CPU \u003d torch.device(\u0027cpu\u0027)\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -\u003e D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id \u003d min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f\u0027cuda:{device_id}\u0027)\n",
        "\n",
        "\n",
        "CUDA \u003d get_device\n",
        "\n",
        "current_directory \u003d os.getcwd()\n",
        "save_path \u003d os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "os.makedirs(save_path, exist_ok\u003dTrue)\n",
        "model_path \u003d os.path.join(save_path, \u0027model_wieghts.pt\u0027)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ClW2ebek8DK",
        "cellView": "form",
        "pycharm": {}
      },
      "source": [
        "#@title Model\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -\u003e T:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias\u003dTrue, act\u003dnn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers \u003d []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias\u003dbias))\n",
        "            if i \u003c len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model \u003d nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -\u003e T:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype\u003dtorch.int64, device\u003ddevice)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] \u003d None, labels: Optional[T] \u003d None):\n",
        "        embedding_text \u003d self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections \u003d self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat \u003d torch.cat((prefix_projections, embedding_text), dim\u003d1)\n",
        "        if labels is not None:\n",
        "            dummy_token \u003d self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels \u003d torch.cat((dummy_token, tokens), dim\u003d1)\n",
        "        out \u003d self.gpt(inputs_embeds\u003dembedding_cat, labels\u003dlabels, attention_mask\u003dmask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int \u003d 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length \u003d prefix_length\n",
        "        self.gpt \u003d GPT2LMHeadModel.from_pretrained(\u0027gpt2\u0027)\n",
        "        self.gpt_embedding_size \u003d self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length \u003e 10:  # not enough memory\n",
        "            self.clip_project \u003d nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project \u003d MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool \u003d True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool \u003d True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7xocT3TUgey",
        "cellView": "form",
        "pycharm": {}
      },
      "source": [
        "#@title Caption prediction\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int \u003d 5, prompt\u003dNone, embed\u003dNone,\n",
        "                  entry_length\u003d67, temperature\u003d1., stop_token: str \u003d \u0027.\u0027):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index \u003d tokenizer.encode(stop_token)[0]\n",
        "    tokens \u003d None\n",
        "    scores \u003d None\n",
        "    device \u003d next(model.parameters()).device\n",
        "    seq_lengths \u003d torch.ones(beam_size, device\u003ddevice)\n",
        "    is_stopped \u003d torch.zeros(beam_size, device\u003ddevice, dtype\u003dtorch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated \u003d embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens \u003d torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens \u003d tokens.unsqueeze(0).to(device)\n",
        "                generated \u003d model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs \u003d model.gpt(inputs_embeds\u003dgenerated)\n",
        "            logits \u003d outputs.logits\n",
        "            logits \u003d logits[:, -1, :] / (temperature if temperature \u003e 0 else 1.0)\n",
        "            logits \u003d logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens \u003d logits.topk(beam_size, -1)\n",
        "                generated \u003d generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores \u003d next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens \u003d next_tokens\n",
        "                else:\n",
        "                    tokens \u003d tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens \u003d torch.cat((tokens, next_tokens), dim\u003d1)\n",
        "            else:\n",
        "                logits[is_stopped] \u003d -float(np.inf)\n",
        "                logits[is_stopped, 0] \u003d 0\n",
        "                scores_sum \u003d scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] +\u003d 1\n",
        "                scores_sum_average \u003d scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens \u003d scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source \u003d next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths \u003d seq_lengths[next_tokens_source]\n",
        "                next_tokens \u003d next_tokens % scores_sum.shape[1]\n",
        "                next_tokens \u003d next_tokens.unsqueeze(1)\n",
        "                tokens \u003d tokens[next_tokens_source]\n",
        "                tokens \u003d torch.cat((tokens, next_tokens), dim\u003d1)\n",
        "                generated \u003d generated[next_tokens_source]\n",
        "                scores \u003d scores_sum_average * seq_lengths\n",
        "                is_stopped \u003d is_stopped[next_tokens_source]\n",
        "            next_token_embed \u003d model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated \u003d torch.cat((generated, next_token_embed), dim\u003d1)\n",
        "            is_stopped \u003d is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores \u003d scores / seq_lengths\n",
        "    output_list \u003d tokens.cpu().numpy()\n",
        "    output_texts \u003d [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order \u003d scores.argsort(descending\u003dTrue)\n",
        "    output_texts \u003d [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens\u003dNone,\n",
        "        prompt\u003dNone,\n",
        "        embed\u003dNone,\n",
        "        entry_count\u003d1,\n",
        "        entry_length\u003d67,  # maximum number of words\n",
        "        top_p\u003d0.8,\n",
        "        temperature\u003d1.,\n",
        "        stop_token: str \u003d \u0027.\u0027,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num \u003d 0\n",
        "    generated_list \u003d []\n",
        "    stop_token_index \u003d tokenizer.encode(stop_token)[0]\n",
        "    filter_value \u003d -float(\"Inf\")\n",
        "    device \u003d next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated \u003d embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens \u003d torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens \u003d tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated \u003d model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs \u003d model.gpt(inputs_embeds\u003dgenerated)\n",
        "                logits \u003d outputs.logits\n",
        "                logits \u003d logits[:, -1, :] / (temperature if temperature \u003e 0 else 1.0)\n",
        "                sorted_logits, sorted_indices \u003d torch.sort(logits, descending\u003dTrue)\n",
        "                cumulative_probs \u003d torch.cumsum(nnf.softmax(sorted_logits, dim\u003d-1), dim\u003d-1)\n",
        "                sorted_indices_to_remove \u003d cumulative_probs \u003e top_p\n",
        "                sorted_indices_to_remove[..., 1:] \u003d sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] \u003d 0\n",
        "\n",
        "                indices_to_remove \u003d sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] \u003d filter_value\n",
        "                next_token \u003d torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed \u003d model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens \u003d next_token\n",
        "                else:\n",
        "                    tokens \u003d torch.cat((tokens, next_token), dim\u003d1)\n",
        "                generated \u003d torch.cat((generated, next_token_embed), dim\u003d1)\n",
        "                if stop_token_index \u003d\u003d next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list \u003d list(tokens.squeeze().cpu().numpy())\n",
        "            output_text \u003d tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE-uUStuv1Nl",
        "cellView": "form",
        "pycharm": {}
      },
      "source": "#@title Choose pretrained model - COCO or Coneptual captions\n\n\npretrained_model \u003d \u0027Conceptual captions\u0027  # @param [\u0027COCO\u0027, \u0027Conceptual captions\u0027]\n\nif pretrained_model \u003d\u003d \u0027Conceptual captions\u0027:\n  downloader.download_file(\"1Xoka96qpJCQAQl21HahTYUvzBvPQY8oa\", model_path)\nelse:\n  downloader.download_file(\"1FcWaTKw9SuXUwUOzn40u9dKiJwvZfBDP\", model_path)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7lCgFHSgr_ny",
        "pycharm": {}
      },
      "source": [
        "#@title GPU/CPU\n",
        "\n",
        "\n",
        "is_gpu \u003d True #@param {type:\"boolean\"}  \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bi_2zQ3QD57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "cellView": "form",
        "outputId": "47afad0d-a76c-4316-8f6d-682dd3e49587",
        "pycharm": {}
      },
      "source": [
        "#@title CLIP model + GPT2 tokenizer\n",
        "\n",
        "device \u003d CUDA(0) if is_gpu else \"cpu\"\n",
        "clip_model, preprocess \u003d clip.load(\"ViT-B/32\", device\u003ddevice, jit\u003dFalse)\n",
        "tokenizer \u003d GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02\u003c00:00, 120MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349a53ebb2ca49da9a9da9f0c0a3160b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00\u003c?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de8b6fae148749058dca73c03955e87b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00\u003c?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c2c9e697f364598855a25c25416f4f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00\u003c?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "954ffefdaa1d4f38aa12839bc7c0b6fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00\u003c?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1eb939f6352f4063808350cc4a97beae",
            "56c9a56fb276438ab3f6b120193deb42",
            "873ac85e2658404ea6b284707824a07d",
            "bcd00181175a4b739afc7a5eb30a235b",
            "2094ef62bd9547fcb133f565818495fa",
            "5a2eb4632f5d477182fcbf0ebcd7eb5b",
            "93c09ca8d2ab40d381163b41c3bd345d",
            "f0585589cf7b49098b0905ed10d96ecf",
            "90bddc0c0d164e93996d0340f45bb9b5",
            "5dc2c242f1594631a573bdbe4ace5a97",
            "7ab34ffb29124fe9a2bf8b2aadf8c514"
          ]
        },
        "id": "glBzYsgIwhwF",
        "cellView": "form",
        "outputId": "2d7637a4-1c76-44b1-c0aa-80a308ee0717",
        "pycharm": {}
      },
      "source": [
        "#@title Load model weights\n",
        "\n",
        "\n",
        "prefix_length \u003d 10\n",
        "\n",
        "model \u003d ClipCaptionModel(prefix_length)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location\u003dCPU)) \n",
        "\n",
        "model \u003d model.eval() \n",
        "device \u003d CUDA(0) if is_gpu else \"cpu\"\n",
        "model \u003d model.to(device)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eb939f6352f4063808350cc4a97beae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/523M [00:00\u003c?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "m5jPDsEA5Kub",
        "cellView": "form",
        "outputId": "5e804e79-e418-47e5-dc96-b2079159e722",
        "pycharm": {}
      },
      "source": [
        "#@title Upload Image\n",
        "\n",
        "\n",
        "uploaded \u003d files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "  UPLOADED_FILE \u003d \u0027\u0027\n",
        "elif len(uploaded) \u003d\u003d 1:\n",
        "  UPLOADED_FILE \u003d list(uploaded.keys())[0]\n",
        "else:\n",
        "  raise AssertionError(\u0027Please upload one image at a time\u0027)\n",
        "\n",
        "print(UPLOADED_FILE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     \u003cinput type\u003d\"file\" id\u003d\"files-9502113e-c525-4b00-a2a2-9cf0a6c7776e\" name\u003d\"files[]\" multiple disabled\n",
              "        style\u003d\"border:none\" /\u003e\n",
              "     \u003coutput id\u003d\"result-9502113e-c525-4b00-a2a2-9cf0a6c7776e\"\u003e\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      \u003c/output\u003e\n",
              "      \u003cscript src\u003d\"/nbextensions/google.colab/files.js\"\u003e\u003c/script\u003e "
            ],
            "text/plain": [
              "\u003cIPython.core.display.HTML object\u003e"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving COCO_val2014_000000354533.jpg to COCO_val2014_000000354533.jpg\n",
            "COCO_val2014_000000354533.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pohtQ8AfWNk_",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5116f31c-1e23-4e2a-9e05-5d8ed1ab835e",
        "pycharm": {}
      },
      "source": "#@title Or download random samples form COCO test set (Karpathy et al. split)\n\nIMAGE_NAME \u003d \u0027354533\u0027  # @param [\u0027562207\u0027, \u0027579664\u0027, \u0027060623\u0027, \u0027165547\u0027, \u0027334321\u0027, \u0027483108\u0027, \u0027386164\u0027, \u0027354533\u0027]\n\nname_ \u003d \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\nimages_path \u003d os.path.join(os.path.dirname(current_directory), \"images\")\nos.makedirs(images_path, exist_ok\u003dTrue)\nUPLOADED_FILE \u003d os.path.join(images_path, name_)\n\nif not os.path.isfile(UPLOADED_FILE):\n  download_path \u003d os.path.join(images_path, \"images.zip\")\n  downloader.download_file(\"1l6J9WFYxpF-1HFr3A5Oq1eoObTxzbPgs\", download_path)\n\n  !unzip {download_path} -d {images_path}\n\n",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /images/images.zip\n",
            "  inflating: /images/COCO_val2014_000000060623.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000060623.jpg  \n",
            "  inflating: /images/COCO_val2014_000000165547.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000165547.jpg  \n",
            "  inflating: /images/COCO_val2014_000000334321.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000334321.jpg  \n",
            "  inflating: /images/COCO_val2014_000000354533.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000354533.jpg  \n",
            "  inflating: /images/COCO_val2014_000000386164.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000386164.jpg  \n",
            "  inflating: /images/COCO_val2014_000000483108.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000483108.jpg  \n",
            "  inflating: /images/COCO_val2014_000000562207.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000562207.jpg  \n",
            "  inflating: /images/COCO_val2014_000000579664.jpg  \n",
            "  inflating: /images/__MACOSX/._COCO_val2014_000000579664.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyVkuZ07llSC",
        "pycharm": {}
      },
      "source": "Conceptual captions examples:\nhttps://drive.google.com/file/d/1ynyxHWSgZ5P1wqSh_RFoXSt16KAoZqu7/view?usp\u003dsharing#%%\n#@title Inference\nuse_beam_search \u003d False #@param {type:\"boolean\"}  \n\nimage \u003d io.imread(UPLOADED_FILE)\npil_image \u003d PIL.Image.fromarray(image)\n#pil_img \u003d Image(filename\u003dUPLOADED_FILE)\ndisplay(pil_image)\n\nimage \u003d preprocess(pil_image).unsqueeze(0).to(device)\nwith torch.no_grad():\n    # if type(model) is ClipCaptionE2E:\n    #     prefix_embed \u003d model.forward_image(image)\n    # else:\n    prefix \u003d clip_model.encode_image(image).to(device, dtype\u003dtorch.float32)\n    prefix_embed \u003d model.clip_project(prefix).reshape(1, prefix_length, -1)\nif use_beam_search:\n    generated_text_prefix \u003d generate_beam(model, tokenizer, embed\u003dprefix_embed)[0]\nelse:\n    generated_text_prefix \u003d generate2(model, tokenizer, embed\u003dprefix_embed)\n\n\nprint(\u0027\\n\u0027)\nprint(generated_text_prefix)"
    }
  ]
}